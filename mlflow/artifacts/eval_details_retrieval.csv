question,answer,ground_truth,contexts,user_input,retrieved_contexts,reference_contexts,response,reference,context_recall,context_precision
What is the role and importance of the 'Dry_Clover_g' component in pasture modeling?,"Dry_Clover_g captures the nitrogen-fixing clover component, which is essential for maintaining pasture nutrition.","Dry_Clover_g captures the nitrogen-fixing clover component, which is essential for maintaining pasture nutrition.","['grams\n✓\n✓\nDry_Total_g\nTotal biomass (all components)\ngrams\n✓\n✓\nThe five biomass target variables represent the core prediction objectives for machine learning models. Dry_Green_g\nquantifies non-legume green vegetation; Dry_Dead_g measures senescent material crucial for feed quality assessment;\nDry_Clover_g captures the nitrogen-fixing clover component, important for pasture nutrition; GDM_g represents the\ncombined green dry matter, representing the most digestible portion of the pasture; and Dry_Total_g denotes the total\nbiomass available for livestock consumption. This component-wise breakdown enables nuanced modeling of pasture\ncomposition beyond simple total biomass estimation.\n4.5\nEvaluation Protocol\nModel performance is evaluated using weighted R2 scores, designed to reflect the relative importance of different', 'that the evaluation metrics are more stable across different scales of biomass measurements and reduces sensitivity to\nextreme values that may occur in natural pasture environments.\n5\nDiscussion\nThis dataset addresses critical gaps in agricultural computer vision by providing the first large-scale collection of pasture\nimages paired with laboratory-validated, component-wise biomass measurements. Unlike existing datasets that rely on\nestimated or visually assessed biomass values, our ground-truth measurements undergo rigorous laboratory validation,\noffering reliable training targets for machine learning models. The detailed breakdown into green, dead, and clover\nfractions enables nuanced modeling of pasture composition, extending beyond total biomass estimation to support feed\nquality assessment.\nThe dataset supports a wide range of research applications, including pasture quality assessment, nitrogen dynamics', '• Dry_Green_g: Green vegetation other than clover (grams)\n• Dry_Dead_g: Senescent material (grams)\n• Dry_Clover_g: Clover component (grams)\n• GDM_g: Green dry matter, calculated as the sum of green vegetation and clover (grams)\n• Dry_Total_g: Total biomass, combining all components (grams)\nThese measurements retain the original quadrat-scale units rather than being extrapolated to per-hectare values, ensuring\ndirect correspondence between image content and biomass labels.\nData integration involved systematically matching image files with corresponding measurement records using unique\nsample identifiers. The final dataset was formatted as standardized CSV files with consistent column naming conventions\nto support machine learning model development and ensure reproducibility across research groups and computational\nenvironments.\n4.4\nLabel Structure\nAll measurements in the proposed dataset represent dry matter content within the standardized 70cm × 30cm quadrat', 'biomass annotations for grass-clover mixtures. However, the Image2Biomass Pasture Innovation Challenge dataset\npresented here has several important differentiators compared to the GrassClover dataset. This dataset is larger in\nscale, offering 1,162 annotated images versus 435, and has additional metadata (species identification, seasonal context,\nlocation details, height measurements, and NDVI data). In terms of temporal coverage, the GrassClover dataset is\nconcentrated between May and October, whereas our dataset spans the entire year. Geographically, it is restricted\nto two locations in Denmark, while our dataset covers 19 sites across four Australian states, representing a wider\nrange of climatic conditions. Additionally, the Image2Biomass Pasture Innovation Challenge dataset includes separate\nannotations for dead matter. Finally, while the GrassClover images were captured using professional cameras under', 'diverse pasture systems [2].\nRemote sensing approaches using satellite and drone imagery, combined with machine learning, enable large-scale\nbiomass estimation. However, they require technical expertise, significant computational resources, and extensive\nground-truth calibration data [12].\n2.2\nComputer Vision Approaches and Existing Datasets\nRecent advances in computer vision have shown promising results for agricultural applications, with deep learning\nmodels demonstrating success in crop yield prediction and vegetation analysis. However, pasture systems present\nunique challenges including species diversity, structural complexity, and environmental variability that make them more\nchallenging than monoculture crops.\nThe most relevant prior work is the GrassClover dataset by Skovsen et al. [13, 14], which provides 435 images with\nbiomass annotations for grass-clover mixtures. However, the Image2Biomass Pasture Innovation Challenge dataset']",What is the role and importance of the 'Dry_Clover_g' component in pasture modeling?,"['grams\n✓\n✓\nDry_Total_g\nTotal biomass (all components)\ngrams\n✓\n✓\nThe five biomass target variables represent the core prediction objectives for machine learning models. Dry_Green_g\nquantifies non-legume green vegetation; Dry_Dead_g measures senescent material crucial for feed quality assessment;\nDry_Clover_g captures the nitrogen-fixing clover component, important for pasture nutrition; GDM_g represents the\ncombined green dry matter, representing the most digestible portion of the pasture; and Dry_Total_g denotes the total\nbiomass available for livestock consumption. This component-wise breakdown enables nuanced modeling of pasture\ncomposition beyond simple total biomass estimation.\n4.5\nEvaluation Protocol\nModel performance is evaluated using weighted R2 scores, designed to reflect the relative importance of different', 'that the evaluation metrics are more stable across different scales of biomass measurements and reduces sensitivity to\nextreme values that may occur in natural pasture environments.\n5\nDiscussion\nThis dataset addresses critical gaps in agricultural computer vision by providing the first large-scale collection of pasture\nimages paired with laboratory-validated, component-wise biomass measurements. Unlike existing datasets that rely on\nestimated or visually assessed biomass values, our ground-truth measurements undergo rigorous laboratory validation,\noffering reliable training targets for machine learning models. The detailed breakdown into green, dead, and clover\nfractions enables nuanced modeling of pasture composition, extending beyond total biomass estimation to support feed\nquality assessment.\nThe dataset supports a wide range of research applications, including pasture quality assessment, nitrogen dynamics', '• Dry_Green_g: Green vegetation other than clover (grams)\n• Dry_Dead_g: Senescent material (grams)\n• Dry_Clover_g: Clover component (grams)\n• GDM_g: Green dry matter, calculated as the sum of green vegetation and clover (grams)\n• Dry_Total_g: Total biomass, combining all components (grams)\nThese measurements retain the original quadrat-scale units rather than being extrapolated to per-hectare values, ensuring\ndirect correspondence between image content and biomass labels.\nData integration involved systematically matching image files with corresponding measurement records using unique\nsample identifiers. The final dataset was formatted as standardized CSV files with consistent column naming conventions\nto support machine learning model development and ensure reproducibility across research groups and computational\nenvironments.\n4.4\nLabel Structure\nAll measurements in the proposed dataset represent dry matter content within the standardized 70cm × 30cm quadrat', 'biomass annotations for grass-clover mixtures. However, the Image2Biomass Pasture Innovation Challenge dataset\npresented here has several important differentiators compared to the GrassClover dataset. This dataset is larger in\nscale, offering 1,162 annotated images versus 435, and has additional metadata (species identification, seasonal context,\nlocation details, height measurements, and NDVI data). In terms of temporal coverage, the GrassClover dataset is\nconcentrated between May and October, whereas our dataset spans the entire year. Geographically, it is restricted\nto two locations in Denmark, while our dataset covers 19 sites across four Australian states, representing a wider\nrange of climatic conditions. Additionally, the Image2Biomass Pasture Innovation Challenge dataset includes separate\nannotations for dead matter. Finally, while the GrassClover images were captured using professional cameras under', 'diverse pasture systems [2].\nRemote sensing approaches using satellite and drone imagery, combined with machine learning, enable large-scale\nbiomass estimation. However, they require technical expertise, significant computational resources, and extensive\nground-truth calibration data [12].\n2.2\nComputer Vision Approaches and Existing Datasets\nRecent advances in computer vision have shown promising results for agricultural applications, with deep learning\nmodels demonstrating success in crop yield prediction and vegetation analysis. However, pasture systems present\nunique challenges including species diversity, structural complexity, and environmental variability that make them more\nchallenging than monoculture crops.\nThe most relevant prior work is the GrassClover dataset by Skovsen et al. [13, 14], which provides 435 images with\nbiomass annotations for grass-clover mixtures. However, the Image2Biomass Pasture Innovation Challenge dataset']","['Dry_Clover_g represents the dry-matter clover (legume) fraction. Clover is modeled separately because it is nutritionally important and contributes via nitrogen fixation, influencing pasture productivity and composition.']","Dry_Clover_g captures the nitrogen-fixing clover component, which is essential for maintaining pasture nutrition.","Dry_Clover_g captures the nitrogen-fixing clover component, which is essential for maintaining pasture nutrition.",1.0,0.9999999999
Over what time period and in which regions was the data collection conducted?,"Data was collected over a three-year period from 2014 to 2017 across 19 locations in four Australian states: New South Wales, Victoria, Tasmania, and Western Australia.","Data was collected over a three-year period from 2014 to 2017 across 19 locations in four Australian states: New South Wales, Victoria, Tasmania, and Western Australia.","['crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.\nVideos associated with this dataset were collected between December 2020 and September 2021. Tracking and annotation was performed\nin October 2021 - February 2022.\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes,\nincluding the outcomes, as well as a link or other access point to any supporting documentation.\nNo.\nDoes the dataset relate to people? If not, you may skip the remaining questions in this section.\nNo.\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\nn/a\nWere the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was', 'identification, such as social security numbers; criminal history)? If so, please provide a description.\nn/a\nAny other comments?\nNone.\nCollection Process\nHow was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey\nresponses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects\nor indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\nSee above for details on collection process. All data pertains to groups of interacting flies in carefully controlled environments.\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program,\nsoftware API)? How were these mechanisms or procedures validated?', 'dataset? If so, please describe how.\nn/a\nDoes the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations,\nreligious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government\nidentification, such as social security numbers; criminal history)? If so, please provide a description.\nn/a\nAny other comments?\nNone.\nCollection Process\nHow was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey\nresponses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects\nor indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\n26', 'portion of the light/dark cycle.\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were\ncrowdworkers paid)?\nBehavioral data collection was performed by graduate student, postdoc, and technician members of the Kumar lab at Jackson Laboratories,\nas a part of another ongoing research project studying animal gait and behavior. (No videos or annotations were explicitly generated\nfor this dataset release.) Lab members are full-time employees of Jackson Labs, and their compensation was not dependent on their\nparticipation in this project. Manual annotation of animal behavior was performed by Markus Marks, who is a full-time employee of\nCaltech and whose compensation was also not dependent on participation in this project.\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent', 'Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were\ncrowdworkers paid)?\nAll data was collected and annotated by Julian Wagner, a graduate student in the lab of Joseph Parker, as part of their thesis work studying\nsocial symbiotic beetles. As a full-time employee of the Parker lab, Wagner’s compensation was not dependent on participation in this\nproject.\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent\ncrawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.\nVideo data was collected and annotated over the course of several months in 2021.\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes,']",Over what time period and in which regions was the data collection conducted?,"['crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.\nVideos associated with this dataset were collected between December 2020 and September 2021. Tracking and annotation was performed\nin October 2021 - February 2022.\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes,\nincluding the outcomes, as well as a link or other access point to any supporting documentation.\nNo.\nDoes the dataset relate to people? If not, you may skip the remaining questions in this section.\nNo.\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\nn/a\nWere the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was', 'identification, such as social security numbers; criminal history)? If so, please provide a description.\nn/a\nAny other comments?\nNone.\nCollection Process\nHow was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey\nresponses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects\nor indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\nSee above for details on collection process. All data pertains to groups of interacting flies in carefully controlled environments.\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program,\nsoftware API)? How were these mechanisms or procedures validated?', 'dataset? If so, please describe how.\nn/a\nDoes the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations,\nreligious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government\nidentification, such as social security numbers; criminal history)? If so, please provide a description.\nn/a\nAny other comments?\nNone.\nCollection Process\nHow was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey\nresponses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects\nor indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\n26', 'portion of the light/dark cycle.\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were\ncrowdworkers paid)?\nBehavioral data collection was performed by graduate student, postdoc, and technician members of the Kumar lab at Jackson Laboratories,\nas a part of another ongoing research project studying animal gait and behavior. (No videos or annotations were explicitly generated\nfor this dataset release.) Lab members are full-time employees of Jackson Labs, and their compensation was not dependent on their\nparticipation in this project. Manual annotation of animal behavior was performed by Markus Marks, who is a full-time employee of\nCaltech and whose compensation was also not dependent on participation in this project.\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent', 'Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were\ncrowdworkers paid)?\nAll data was collected and annotated by Julian Wagner, a graduate student in the lab of Joseph Parker, as part of their thesis work studying\nsocial symbiotic beetles. As a full-time employee of the Parker lab, Wagner’s compensation was not dependent on participation in this\nproject.\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent\ncrawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.\nVideo data was collected and annotated over the course of several months in 2021.\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes,']","['Data collection spanned three years (2014–2017) across 19 sites in four Australian states: New South Wales, Victoria, Tasmania, and Western Australia.']","Data was collected over a three-year period from 2014 to 2017 across 19 locations in four Australian states: New South Wales, Victoria, Tasmania, and Western Australia.","Data was collected over a three-year period from 2014 to 2017 across 19 locations in four Australian states: New South Wales, Victoria, Tasmania, and Western Australia.",0.0,0.0
Why was sampling avoided immediately after rainfall events?,Sampling was avoided after rain to prevent soil contamination and to maintain consistent measurement conditions.,Sampling was avoided after rain to prevent soil contamination and to maintain consistent measurement conditions.,"['If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\nThis answer is repeated from an earlier section: the source dataset consists of 2-hour-long videos of rove beetle-interactor pairings, with\neach video capturing eight such pairings simultaneously (housed within the wells of an eight-well plate.) The raw video recordings\nwere screened to identify wells that appeared to have occurrences of multiple types of behavior of interest; manual annotation of animal\nbehavior was performed on this subset of wells. It is therefore possible that this dataset is biased for videos with higher rates of animal\nmovement than in the full raw video dataset; this was done to provide a larger number of representative examples of animal behavior.\nThe 30 second clips comprising each instance in this dataset are extracted from the subset of wells for which annotation was performed.', 'Livestock Australia (MLA)’s Participatory Research Groups to ensure representation of commercially relevant pasture\ntypes and management practices.\nEach sampling campaign adhered to a rigorous, standardized protocol to ensure data quality and consistency across all\nsites and seasons. Representative areas within paddocks were carefully selected to avoid edge effects, and transects\nwere established to capture within-paddock variability. Weather conditions were systematically recorded, and sampling\nwas deliberately avoided immediately following rainfall events to prevent soil contamination and maintain consistent\nmeasurement conditions.\nThe data collection sequence began with precise quadrat placement using a standardized 70cm × 30cm metal frame\npositioned directly on the pasture surface. High-resolution photographs were taken from above using a variety of camera\nsystems (see Table 1) to accommodate differing field conditions and research team setups. NDVI measurements were\n3', 'images to confirm complete sampling and identify any procedural errors. Outlier detection protocols flagged samples\nwith unusually high biomass-to-height ratios, which may indicate soil contamination, while NDVI readings outside the\ntypical vegetation range (0.1-0.9) were investigated for potential sensor errors or environmental anomalies.\nCross-validation procedures were used to compare height and biomass measurements, helping to identify inconsistencies,\nand completeness checks ensured that all required data components were present for each sample. Expert review of\nspecies identification was conducted for mixed swards to verify botanical accuracy. Of the initial collection of 3,187\nsamples, 1,162 passed the comprehensive quality control process. Exclusions were primarily due to incomplete cutting\n(i.e. vegetation remaining in quadrat after harvest), soil contamination evidenced by unusually high mineral content,', 'Figure 2: Geographic distribution of sampling sites across Australia [2]\nobtained using a GreenSeeker handheld sensor positioned approximately 1m above the center of each quadrat, with an\naverage of 100 readings recorded to account for sensor variability and ensure measurement reliability. Compressed\nheight measurements were collected using a falling plate meter (30cm diameter, 200g weight), with multiple readings\ntaken within each quadrat and averaged to provide representative height values.\nFollowing nondestructive measurements, all vegetation within the quadrat was harvested by cutting to ground level\nusing electric clippers. The harvested material was immediately collected in labeled paper bags to prevent sample loss\nand maintain traceability. Post-harvest photographs and NDVI readings were taken to document complete vegetation\nremoval and provide quality control data.\n3.2\nLaboratory Processing and Quality Control', 'removal and provide quality control data.\n3.2\nLaboratory Processing and Quality Control\nAll samples underwent standardized laboratory processing within 24 hours of collection to minimize degradation and\nensure accurate measurements. Fresh weight was recorded for each complete sample, followed by manual sorting of a\nminimum 30g subsample into three distinct components: green material (living, non-legume vegetation), dead material\n(senescent or brown vegetation), and clover, which was separated due to its nutritional significance and nitrogen-fixing\nproperties. This component-wise sorting enables detailed inference of pasture composition and quality, which is\nessential for accurate livestock nutrition assessment.\nAll fractions were dried at 70°C for 48 hours following standard protocols, with individual component weights recorded\nto calculate dry matter content. Final biomass values were calculated per hectare based on the known quadrat area,']",Why was sampling avoided immediately after rainfall events?,"['If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\nThis answer is repeated from an earlier section: the source dataset consists of 2-hour-long videos of rove beetle-interactor pairings, with\neach video capturing eight such pairings simultaneously (housed within the wells of an eight-well plate.) The raw video recordings\nwere screened to identify wells that appeared to have occurrences of multiple types of behavior of interest; manual annotation of animal\nbehavior was performed on this subset of wells. It is therefore possible that this dataset is biased for videos with higher rates of animal\nmovement than in the full raw video dataset; this was done to provide a larger number of representative examples of animal behavior.\nThe 30 second clips comprising each instance in this dataset are extracted from the subset of wells for which annotation was performed.', 'Livestock Australia (MLA)’s Participatory Research Groups to ensure representation of commercially relevant pasture\ntypes and management practices.\nEach sampling campaign adhered to a rigorous, standardized protocol to ensure data quality and consistency across all\nsites and seasons. Representative areas within paddocks were carefully selected to avoid edge effects, and transects\nwere established to capture within-paddock variability. Weather conditions were systematically recorded, and sampling\nwas deliberately avoided immediately following rainfall events to prevent soil contamination and maintain consistent\nmeasurement conditions.\nThe data collection sequence began with precise quadrat placement using a standardized 70cm × 30cm metal frame\npositioned directly on the pasture surface. High-resolution photographs were taken from above using a variety of camera\nsystems (see Table 1) to accommodate differing field conditions and research team setups. NDVI measurements were\n3', 'images to confirm complete sampling and identify any procedural errors. Outlier detection protocols flagged samples\nwith unusually high biomass-to-height ratios, which may indicate soil contamination, while NDVI readings outside the\ntypical vegetation range (0.1-0.9) were investigated for potential sensor errors or environmental anomalies.\nCross-validation procedures were used to compare height and biomass measurements, helping to identify inconsistencies,\nand completeness checks ensured that all required data components were present for each sample. Expert review of\nspecies identification was conducted for mixed swards to verify botanical accuracy. Of the initial collection of 3,187\nsamples, 1,162 passed the comprehensive quality control process. Exclusions were primarily due to incomplete cutting\n(i.e. vegetation remaining in quadrat after harvest), soil contamination evidenced by unusually high mineral content,', 'Figure 2: Geographic distribution of sampling sites across Australia [2]\nobtained using a GreenSeeker handheld sensor positioned approximately 1m above the center of each quadrat, with an\naverage of 100 readings recorded to account for sensor variability and ensure measurement reliability. Compressed\nheight measurements were collected using a falling plate meter (30cm diameter, 200g weight), with multiple readings\ntaken within each quadrat and averaged to provide representative height values.\nFollowing nondestructive measurements, all vegetation within the quadrat was harvested by cutting to ground level\nusing electric clippers. The harvested material was immediately collected in labeled paper bags to prevent sample loss\nand maintain traceability. Post-harvest photographs and NDVI readings were taken to document complete vegetation\nremoval and provide quality control data.\n3.2\nLaboratory Processing and Quality Control', 'removal and provide quality control data.\n3.2\nLaboratory Processing and Quality Control\nAll samples underwent standardized laboratory processing within 24 hours of collection to minimize degradation and\nensure accurate measurements. Fresh weight was recorded for each complete sample, followed by manual sorting of a\nminimum 30g subsample into three distinct components: green material (living, non-legume vegetation), dead material\n(senescent or brown vegetation), and clover, which was separated due to its nutritional significance and nitrogen-fixing\nproperties. This component-wise sorting enables detailed inference of pasture composition and quality, which is\nessential for accurate livestock nutrition assessment.\nAll fractions were dried at 70°C for 48 hours following standard protocols, with individual component weights recorded\nto calculate dry matter content. Final biomass values were calculated per hectare based on the known quadrat area,']",['Sampling was avoided immediately after rainfall to reduce soil contamination risk and to keep sampling/measurement conditions consistent.'],Sampling was avoided after rain to prevent soil contamination and to maintain consistent measurement conditions.,Sampling was avoided after rain to prevent soil contamination and to maintain consistent measurement conditions.,1.0,0.49999999995
What is the specific technical challenge associated with using capacitance meters?,"Capacitance meters require complex, pasture-specific calibration and often yield inconsistent results across different environmental conditions.","Capacitance meters require complex, pasture-specific calibration and often yield inconsistent results across different environmental conditions.","['2\nRelated Work\n2.1\nTraditional Biomass Measurement Methods\nCurrent approaches to estimating pasture biomass have significant limitations that restrict their practical applications.\nDestructive methods such as clipping and weighing (cut-dry-weigh method) provide high accuracy through direct\nmeasurement but are labor-intensive, time-consuming, and impractical for large-scale or frequent monitoring [5].\nNon-destructive mechanical methods include the rising plate meters, which measure compressed pasture height but\nrequire calibration for different pasture types and show variable accuracy with pasture density and moisture content\n[6], and capacitance meters that measure electrical capacitance of vegetation but require complex, pasture-specific\ncalibration with inconsistent results across different conditions [7].\n2', '(i.e. vegetation remaining in quadrat after harvest), soil contamination evidenced by unusually high mineral content,\nmeasurement errors or missing data components, and non-representative species composition that did not align with\ntarget pasture types.\n4.3\nBiomass Measurement Standards and Data Integration\nThe final biomass measurements represent dry matter content collected within a standardized 70cm × 30cm quadrat\nframe, enabling precise component-wise quantification critical for detailed pasture analysis. The measurement protocol\nproduced five key biomass variables:\n5', 'Optical and spectral methods have gained attention because of their potential to provide rapid, nondestructive assessments. Active Optical Sensors (AOS), originally developed for cropping industries to infer nitrogen levels, measure\nvegetation indices like NDVI by directing light beams onto the canopy and calculating optical reflectance [8, 9, 11].\nResearch has shown that combining NDVI measurements with height data from LiDAR or mechanical sensors can\nsignificantly improve the accuracy of biomass estimation, addressing limitations such as NDVI saturation at high\nbiomass levels, typically when Leaf Area Index (LAI) > 3, and height measurement errors caused by lodging or sparse\ncanopies [10]. However, the primary challenge remains the development of robust calibration methods suitable for\ndiverse pasture systems [2].\nRemote sensing approaches using satellite and drone imagery, combined with machine learning, enable large-scale', '[4] Australian\nBureau\nof\nStatistics.\nNational\nland\naccount\nexperimental\nestimates.\nhttps://www.abs.gov.au/statistics/environment/environmental-management/national-land-account-experimentalestimates/latest-release#land-use, 2024. Accessed: October 28, 2025.\n[5] C. Edwards, L. H. McCormick, and W. D. Hoffman. Pasture skills audit – a reflection on extension. In APEN\nNational Forum: hitting a Moving target Sustaining landscapes, livelihoods and lifestyles in a changing world,\nArmidale NSW, 2011. Australasia Pacific Extension Network.\n[6] Ed Rayburn and John Lozier. A falling plate meter for estimating pasture forage mass. 2003.\n[7] M. G. Trotter, D. W. Lamb, G. E. Donald, and D. A. Schneider. Evaluating an active optical sensor for quantifying\nand mapping green herbage mass and growth in a perennial grass pasture. Crop and Pasture Science, 61:389–398,\n2010.\n[8] R. K. Teal, B. Tubana, K. Girma, K. W. Freeman, D. B. Arnall, O. Walsh, and W. R. Raun. In-season prediction of', 'to calculate dry matter content. Final biomass values were calculated per hectare based on the known quadrat area,\nproviding standardized units for model training and evaluation. Rigorous quality control measures included photographic\nverification of pre- and post-harvest conditions, outlier detection for samples showing evidence of soil contamination or\nmeasurement errors, and cross-validation between height and biomass measurements to ensure data integrity.\n4']",What is the specific technical challenge associated with using capacitance meters?,"['2\nRelated Work\n2.1\nTraditional Biomass Measurement Methods\nCurrent approaches to estimating pasture biomass have significant limitations that restrict their practical applications.\nDestructive methods such as clipping and weighing (cut-dry-weigh method) provide high accuracy through direct\nmeasurement but are labor-intensive, time-consuming, and impractical for large-scale or frequent monitoring [5].\nNon-destructive mechanical methods include the rising plate meters, which measure compressed pasture height but\nrequire calibration for different pasture types and show variable accuracy with pasture density and moisture content\n[6], and capacitance meters that measure electrical capacitance of vegetation but require complex, pasture-specific\ncalibration with inconsistent results across different conditions [7].\n2', '(i.e. vegetation remaining in quadrat after harvest), soil contamination evidenced by unusually high mineral content,\nmeasurement errors or missing data components, and non-representative species composition that did not align with\ntarget pasture types.\n4.3\nBiomass Measurement Standards and Data Integration\nThe final biomass measurements represent dry matter content collected within a standardized 70cm × 30cm quadrat\nframe, enabling precise component-wise quantification critical for detailed pasture analysis. The measurement protocol\nproduced five key biomass variables:\n5', 'Optical and spectral methods have gained attention because of their potential to provide rapid, nondestructive assessments. Active Optical Sensors (AOS), originally developed for cropping industries to infer nitrogen levels, measure\nvegetation indices like NDVI by directing light beams onto the canopy and calculating optical reflectance [8, 9, 11].\nResearch has shown that combining NDVI measurements with height data from LiDAR or mechanical sensors can\nsignificantly improve the accuracy of biomass estimation, addressing limitations such as NDVI saturation at high\nbiomass levels, typically when Leaf Area Index (LAI) > 3, and height measurement errors caused by lodging or sparse\ncanopies [10]. However, the primary challenge remains the development of robust calibration methods suitable for\ndiverse pasture systems [2].\nRemote sensing approaches using satellite and drone imagery, combined with machine learning, enable large-scale', '[4] Australian\nBureau\nof\nStatistics.\nNational\nland\naccount\nexperimental\nestimates.\nhttps://www.abs.gov.au/statistics/environment/environmental-management/national-land-account-experimentalestimates/latest-release#land-use, 2024. Accessed: October 28, 2025.\n[5] C. Edwards, L. H. McCormick, and W. D. Hoffman. Pasture skills audit – a reflection on extension. In APEN\nNational Forum: hitting a Moving target Sustaining landscapes, livelihoods and lifestyles in a changing world,\nArmidale NSW, 2011. Australasia Pacific Extension Network.\n[6] Ed Rayburn and John Lozier. A falling plate meter for estimating pasture forage mass. 2003.\n[7] M. G. Trotter, D. W. Lamb, G. E. Donald, and D. A. Schneider. Evaluating an active optical sensor for quantifying\nand mapping green herbage mass and growth in a perennial grass pasture. Crop and Pasture Science, 61:389–398,\n2010.\n[8] R. K. Teal, B. Tubana, K. Girma, K. W. Freeman, D. B. Arnall, O. Walsh, and W. R. Raun. In-season prediction of', 'to calculate dry matter content. Final biomass values were calculated per hectare based on the known quadrat area,\nproviding standardized units for model training and evaluation. Rigorous quality control measures included photographic\nverification of pre- and post-harvest conditions, outlier detection for samples showing evidence of soil contamination or\nmeasurement errors, and cross-validation between height and biomass measurements to ensure data integrity.\n4']","['Capacitance meters require complex, pasture-specific calibration and can produce inconsistent results when environmental and pasture conditions vary.']","Capacitance meters require complex, pasture-specific calibration and often yield inconsistent results across different environmental conditions.","Capacitance meters require complex, pasture-specific calibration and often yield inconsistent results across different environmental conditions.",1.0,0.9999999999
What are the specific temperature and time requirements for drying the pasture samples?,All fractions were dried at 70°C for a duration of 48 hours following standard protocols.,All fractions were dried at 70°C for a duration of 48 hours following standard protocols.,"['removal and provide quality control data.\n3.2\nLaboratory Processing and Quality Control\nAll samples underwent standardized laboratory processing within 24 hours of collection to minimize degradation and\nensure accurate measurements. Fresh weight was recorded for each complete sample, followed by manual sorting of a\nminimum 30g subsample into three distinct components: green material (living, non-legume vegetation), dead material\n(senescent or brown vegetation), and clover, which was separated due to its nutritional significance and nitrogen-fixing\nproperties. This component-wise sorting enables detailed inference of pasture composition and quality, which is\nessential for accurate livestock nutrition assessment.\nAll fractions were dried at 70°C for 48 hours following standard protocols, with individual component weights recorded\nto calculate dry matter content. Final biomass values were calculated per hectare based on the known quadrat area,', 'Livestock Australia (MLA)’s Participatory Research Groups to ensure representation of commercially relevant pasture\ntypes and management practices.\nEach sampling campaign adhered to a rigorous, standardized protocol to ensure data quality and consistency across all\nsites and seasons. Representative areas within paddocks were carefully selected to avoid edge effects, and transects\nwere established to capture within-paddock variability. Weather conditions were systematically recorded, and sampling\nwas deliberately avoided immediately following rainfall events to prevent soil contamination and maintain consistent\nmeasurement conditions.\nThe data collection sequence began with precise quadrat placement using a standardized 70cm × 30cm metal frame\npositioned directly on the pasture surface. High-resolution photographs were taken from above using a variety of camera\nsystems (see Table 1) to accommodate differing field conditions and research team setups. NDVI measurements were\n3', 'identification) [19], DeepWeeds (weed detection) [20], and Agriculture-Vision (aerial field pattern analysis) [21] serve\ndistinct purposes within the agricultural domain. However, they lack the ground-level, multi-modal measurements\nnecessary for detailed pasture biomass estimation across diverse environmental conditions and species compositions.\n3\nData Collection\n3.1\nStudy Sites and Sampling Protocol\nData collection was conducted across 19 locations spanning four Australian states over a three-year period (2014–2017),\nrepresenting a wide range of soil, climate, and seasonal conditions across diverse grazing landscapes in the temperate\npasture zones of southern Australia. Multiple sites were selected across New South Wales, Victoria, Tasmania, and\nWestern Australia (illustrated in Figure 2). Sites were strategically selected in collaboration with a network of Meat &\nLivestock Australia (MLA)’s Participatory Research Groups to ensure representation of commercially relevant pasture', '[4] Australian\nBureau\nof\nStatistics.\nNational\nland\naccount\nexperimental\nestimates.\nhttps://www.abs.gov.au/statistics/environment/environmental-management/national-land-account-experimentalestimates/latest-release#land-use, 2024. Accessed: October 28, 2025.\n[5] C. Edwards, L. H. McCormick, and W. D. Hoffman. Pasture skills audit – a reflection on extension. In APEN\nNational Forum: hitting a Moving target Sustaining landscapes, livelihoods and lifestyles in a changing world,\nArmidale NSW, 2011. Australasia Pacific Extension Network.\n[6] Ed Rayburn and John Lozier. A falling plate meter for estimating pasture forage mass. 2003.\n[7] M. G. Trotter, D. W. Lamb, G. E. Donald, and D. A. Schneider. Evaluating an active optical sensor for quantifying\nand mapping green herbage mass and growth in a perennial grass pasture. Crop and Pasture Science, 61:389–398,\n2010.\n[8] R. K. Teal, B. Tubana, K. Girma, K. W. Freeman, D. B. Arnall, O. Walsh, and W. R. Raun. In-season prediction of', '(i.e. vegetation remaining in quadrat after harvest), soil contamination evidenced by unusually high mineral content,\nmeasurement errors or missing data components, and non-representative species composition that did not align with\ntarget pasture types.\n4.3\nBiomass Measurement Standards and Data Integration\nThe final biomass measurements represent dry matter content collected within a standardized 70cm × 30cm quadrat\nframe, enabling precise component-wise quantification critical for detailed pasture analysis. The measurement protocol\nproduced five key biomass variables:\n5']",What are the specific temperature and time requirements for drying the pasture samples?,"['removal and provide quality control data.\n3.2\nLaboratory Processing and Quality Control\nAll samples underwent standardized laboratory processing within 24 hours of collection to minimize degradation and\nensure accurate measurements. Fresh weight was recorded for each complete sample, followed by manual sorting of a\nminimum 30g subsample into three distinct components: green material (living, non-legume vegetation), dead material\n(senescent or brown vegetation), and clover, which was separated due to its nutritional significance and nitrogen-fixing\nproperties. This component-wise sorting enables detailed inference of pasture composition and quality, which is\nessential for accurate livestock nutrition assessment.\nAll fractions were dried at 70°C for 48 hours following standard protocols, with individual component weights recorded\nto calculate dry matter content. Final biomass values were calculated per hectare based on the known quadrat area,', 'Livestock Australia (MLA)’s Participatory Research Groups to ensure representation of commercially relevant pasture\ntypes and management practices.\nEach sampling campaign adhered to a rigorous, standardized protocol to ensure data quality and consistency across all\nsites and seasons. Representative areas within paddocks were carefully selected to avoid edge effects, and transects\nwere established to capture within-paddock variability. Weather conditions were systematically recorded, and sampling\nwas deliberately avoided immediately following rainfall events to prevent soil contamination and maintain consistent\nmeasurement conditions.\nThe data collection sequence began with precise quadrat placement using a standardized 70cm × 30cm metal frame\npositioned directly on the pasture surface. High-resolution photographs were taken from above using a variety of camera\nsystems (see Table 1) to accommodate differing field conditions and research team setups. NDVI measurements were\n3', 'identification) [19], DeepWeeds (weed detection) [20], and Agriculture-Vision (aerial field pattern analysis) [21] serve\ndistinct purposes within the agricultural domain. However, they lack the ground-level, multi-modal measurements\nnecessary for detailed pasture biomass estimation across diverse environmental conditions and species compositions.\n3\nData Collection\n3.1\nStudy Sites and Sampling Protocol\nData collection was conducted across 19 locations spanning four Australian states over a three-year period (2014–2017),\nrepresenting a wide range of soil, climate, and seasonal conditions across diverse grazing landscapes in the temperate\npasture zones of southern Australia. Multiple sites were selected across New South Wales, Victoria, Tasmania, and\nWestern Australia (illustrated in Figure 2). Sites were strategically selected in collaboration with a network of Meat &\nLivestock Australia (MLA)’s Participatory Research Groups to ensure representation of commercially relevant pasture', '[4] Australian\nBureau\nof\nStatistics.\nNational\nland\naccount\nexperimental\nestimates.\nhttps://www.abs.gov.au/statistics/environment/environmental-management/national-land-account-experimentalestimates/latest-release#land-use, 2024. Accessed: October 28, 2025.\n[5] C. Edwards, L. H. McCormick, and W. D. Hoffman. Pasture skills audit – a reflection on extension. In APEN\nNational Forum: hitting a Moving target Sustaining landscapes, livelihoods and lifestyles in a changing world,\nArmidale NSW, 2011. Australasia Pacific Extension Network.\n[6] Ed Rayburn and John Lozier. A falling plate meter for estimating pasture forage mass. 2003.\n[7] M. G. Trotter, D. W. Lamb, G. E. Donald, and D. A. Schneider. Evaluating an active optical sensor for quantifying\nand mapping green herbage mass and growth in a perennial grass pasture. Crop and Pasture Science, 61:389–398,\n2010.\n[8] R. K. Teal, B. Tubana, K. Girma, K. W. Freeman, D. B. Arnall, O. Walsh, and W. R. Raun. In-season prediction of', '(i.e. vegetation remaining in quadrat after harvest), soil contamination evidenced by unusually high mineral content,\nmeasurement errors or missing data components, and non-representative species composition that did not align with\ntarget pasture types.\n4.3\nBiomass Measurement Standards and Data Integration\nThe final biomass measurements represent dry matter content collected within a standardized 70cm × 30cm quadrat\nframe, enabling precise component-wise quantification critical for detailed pasture analysis. The measurement protocol\nproduced five key biomass variables:\n5']",['All separated sample fractions were dried at 70°C for 48 hours according to standard protocols.'],All fractions were dried at 70°C for a duration of 48 hours following standard protocols.,All fractions were dried at 70°C for a duration of 48 hours following standard protocols.,1.0,0.9999999999
What is the goal of the MABe22 dataset?,The goal of the MABe22 dataset is to benchmark representation learning models for behavior analysis using data from biology experiments.,The goal of the MABe22 dataset is to benchmark representation learning models for behavior analysis using data from biology experiments.,"['The MABe22 dataset is a new animal behavior dataset, intended to a) serve as a benchmark dataset for comparison of unsupervised or\nself-supervised behavior analysis tools, and establish community standards for evaluation of unsupervised techniques, b) highlight critical\nchallenges in computational behavior analysis, particularly pertaining to unsupervised representation learning, and c) foster interaction\nbetween behavioral biologists and the greater machine learning community. This datasheet is specific to the Fly Group dataset, which\nconsists of tracking data for a group of 8 to 11 fruit flies with 50 “hidden labels” for evaluating the quality of the learned representation.\nAlso see MABe22 mouse triplet data sheet (Section B.1) for more details.\nWho created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\nThe MABe22 fly dataset was created as a collaborative effort between Kristin Branson, Alice Robie, and Catherine Schretter at HHMI', 'MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nAppendix for MABe22\nLinks to access our code and dataset, including code from challenge winners where available, are on our dataset website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset. The sections of our appendix are\norganized as follows:\n• Section A contains details of community-contributed methods from our open competition.\n• Section B contains dataset documentation and intended uses for MABe2022, following the format of the Datasheet for\nDatasets(Gebru et al., 2018).\n• Section C contains additional dataset details for mouse, fly, and beetle datasets.\n• Section D shows the evaluation metrics for MABe2022, namely the F1 score and Mean Squared Error.\n• Section E contains additional implementation details of our models.\n• Section F provides additional evaluation results on the trajectory data of MABe22 (mouse and fly).', 'MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nWhile this dataset was designed for development of methods for representation learning, the annotations can also be used for supervised\nlearning tasks.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\nFor example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping,', 'representation of the video. They upload their learned representations to the evaluation site, which then trained simple linear classifiers on\nthe set of secret tasks described above, and returns accuracy measures.\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nBesides unsupervised representation learning, this dataset could also be used for supervised representation learning, using the hidden\nlabels as supervision.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?', 'n/a\nAny other comments?\nNone.\nUses\nHas the dataset been used for any tasks already? If so, please provide a description.\nYes: this dataset was released to accompany the 2022 Multi-Agent Behavior (MABe) Challenge, posted here. This competition was aimed\nat generating learned representations of animals’ actions using unsupervised or self-supervised techniques.\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nWhile this dataset was designed for development of methods for representation learning, the annotations can also be used for supervised\nlearning tasks.\n28']",What is the goal of the MABe22 dataset?,"['The MABe22 dataset is a new animal behavior dataset, intended to a) serve as a benchmark dataset for comparison of unsupervised or\nself-supervised behavior analysis tools, and establish community standards for evaluation of unsupervised techniques, b) highlight critical\nchallenges in computational behavior analysis, particularly pertaining to unsupervised representation learning, and c) foster interaction\nbetween behavioral biologists and the greater machine learning community. This datasheet is specific to the Fly Group dataset, which\nconsists of tracking data for a group of 8 to 11 fruit flies with 50 “hidden labels” for evaluating the quality of the learned representation.\nAlso see MABe22 mouse triplet data sheet (Section B.1) for more details.\nWho created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\nThe MABe22 fly dataset was created as a collaborative effort between Kristin Branson, Alice Robie, and Catherine Schretter at HHMI', 'MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nAppendix for MABe22\nLinks to access our code and dataset, including code from challenge winners where available, are on our dataset website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset. The sections of our appendix are\norganized as follows:\n• Section A contains details of community-contributed methods from our open competition.\n• Section B contains dataset documentation and intended uses for MABe2022, following the format of the Datasheet for\nDatasets(Gebru et al., 2018).\n• Section C contains additional dataset details for mouse, fly, and beetle datasets.\n• Section D shows the evaluation metrics for MABe2022, namely the F1 score and Mean Squared Error.\n• Section E contains additional implementation details of our models.\n• Section F provides additional evaluation results on the trajectory data of MABe22 (mouse and fly).', 'MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nWhile this dataset was designed for development of methods for representation learning, the annotations can also be used for supervised\nlearning tasks.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\nFor example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping,', 'representation of the video. They upload their learned representations to the evaluation site, which then trained simple linear classifiers on\nthe set of secret tasks described above, and returns accuracy measures.\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nBesides unsupervised representation learning, this dataset could also be used for supervised representation learning, using the hidden\nlabels as supervision.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?', 'n/a\nAny other comments?\nNone.\nUses\nHas the dataset been used for any tasks already? If so, please provide a description.\nYes: this dataset was released to accompany the 2022 Multi-Agent Behavior (MABe) Challenge, posted here. This competition was aimed\nat generating learned representations of animals’ actions using unsupervised or self-supervised techniques.\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nWhile this dataset was designed for development of methods for representation learning, the annotations can also be used for supervised\nlearning tasks.\n28']",['MABe22 is designed to benchmark representation learning methods for behavior analysis using data collected from biology experiments.'],The goal of the MABe22 dataset is to benchmark representation learning models for behavior analysis using data from biology experiments.,The goal of the MABe22 dataset is to benchmark representation learning models for behavior analysis using data from biology experiments.,1.0,0.9999999999
What organisms are included in the MABe22 dataset?,"The MABe22 dataset includes data from three model organisms: mice, beetles (rove beetles), and flies (fruit flies).","The MABe22 dataset includes data from three model organisms: mice, beetles (rove beetles), and flies (fruit flies).","['The MABe22 dataset is a new animal behavior dataset, intended to a) serve as a benchmark dataset for comparison of unsupervised or\nself-supervised behavior analysis tools, and establish community standards for evaluation of unsupervised techniques, b) highlight critical\nchallenges in computational behavior analysis, particularly pertaining to unsupervised representation learning, and c) foster interaction\nbetween behavioral biologists and the greater machine learning community. This datasheet is specific to the Fly Group dataset, which\nconsists of tracking data for a group of 8 to 11 fruit flies with 50 “hidden labels” for evaluating the quality of the learned representation.\nAlso see MABe22 mouse triplet data sheet (Section B.1) for more details.\nWho created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\nThe MABe22 fly dataset was created as a collaborative effort between Kristin Branson, Alice Robie, and Catherine Schretter at HHMI', 'MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nAppendix for MABe22\nLinks to access our code and dataset, including code from challenge winners where available, are on our dataset website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset. The sections of our appendix are\norganized as follows:\n• Section A contains details of community-contributed methods from our open competition.\n• Section B contains dataset documentation and intended uses for MABe2022, following the format of the Datasheet for\nDatasets(Gebru et al., 2018).\n• Section C contains additional dataset details for mouse, fly, and beetle datasets.\n• Section D shows the evaluation metrics for MABe2022, namely the F1 score and Mean Squared Error.\n• Section E contains additional implementation details of our models.\n• Section F provides additional evaluation results on the trajectory data of MABe22 (mouse and fly).', 'MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nWhile this dataset was designed for development of methods for representation learning, the annotations can also be used for supervised\nlearning tasks.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\nFor example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping,', 'MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nSequences in the dataset are derived from video of triplets of socially interacting mice in an open arena. Video data was processed to\nextract pose estimates and track identity of the animals, and to generate automated annotations of several behaviors of interest, included in\nthe hidden labels in this dataset.\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program,\nsoftware API)? How were these mechanisms or procedures validated?\nBehavioral data was collected in the JAX Animal Behavior System (Beane et al., 2022). Videos were recorded using Basler acA130075gm camera with Tamron 4-12mm lens and 800nm longpass filter, at a framerate of 30Hz and camera resolution of 800 x 800 pixels.\nThe camera was mounted 105+/-5 cm above the floor of an open field measuring 52cm x 52cm; a grate located at the northern wall of the', 'representation of the video. They upload their learned representations to the evaluation site, which then trained simple linear classifiers on\nthe set of secret tasks described above, and returns accuracy measures.\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nBesides unsupervised representation learning, this dataset could also be used for supervised representation learning, using the hidden\nlabels as supervision.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?']",What organisms are included in the MABe22 dataset?,"['The MABe22 dataset is a new animal behavior dataset, intended to a) serve as a benchmark dataset for comparison of unsupervised or\nself-supervised behavior analysis tools, and establish community standards for evaluation of unsupervised techniques, b) highlight critical\nchallenges in computational behavior analysis, particularly pertaining to unsupervised representation learning, and c) foster interaction\nbetween behavioral biologists and the greater machine learning community. This datasheet is specific to the Fly Group dataset, which\nconsists of tracking data for a group of 8 to 11 fruit flies with 50 “hidden labels” for evaluating the quality of the learned representation.\nAlso see MABe22 mouse triplet data sheet (Section B.1) for more details.\nWho created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\nThe MABe22 fly dataset was created as a collaborative effort between Kristin Branson, Alice Robie, and Catherine Schretter at HHMI', 'MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nAppendix for MABe22\nLinks to access our code and dataset, including code from challenge winners where available, are on our dataset website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset. The sections of our appendix are\norganized as follows:\n• Section A contains details of community-contributed methods from our open competition.\n• Section B contains dataset documentation and intended uses for MABe2022, following the format of the Datasheet for\nDatasets(Gebru et al., 2018).\n• Section C contains additional dataset details for mouse, fly, and beetle datasets.\n• Section D shows the evaluation metrics for MABe2022, namely the F1 score and Mean Squared Error.\n• Section E contains additional implementation details of our models.\n• Section F provides additional evaluation results on the trajectory data of MABe22 (mouse and fly).', 'MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nWhile this dataset was designed for development of methods for representation learning, the annotations can also be used for supervised\nlearning tasks.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\nFor example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping,', 'MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nSequences in the dataset are derived from video of triplets of socially interacting mice in an open arena. Video data was processed to\nextract pose estimates and track identity of the animals, and to generate automated annotations of several behaviors of interest, included in\nthe hidden labels in this dataset.\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program,\nsoftware API)? How were these mechanisms or procedures validated?\nBehavioral data was collected in the JAX Animal Behavior System (Beane et al., 2022). Videos were recorded using Basler acA130075gm camera with Tamron 4-12mm lens and 800nm longpass filter, at a framerate of 30Hz and camera resolution of 800 x 800 pixels.\nThe camera was mounted 105+/-5 cm above the floor of an open field measuring 52cm x 52cm; a grate located at the northern wall of the', 'representation of the video. They upload their learned representations to the evaluation site, which then trained simple linear classifiers on\nthe set of secret tasks described above, and returns accuracy measures.\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nBesides unsupervised representation learning, this dataset could also be used for supervised representation learning, using the hidden\nlabels as supervision.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?']","['MABe22 includes three model organisms: mice, rove beetles, and fruit flies.']","The MABe22 dataset includes data from three model organisms: mice, beetles (rove beetles), and flies (fruit flies).","The MABe22 dataset includes data from three model organisms: mice, beetles (rove beetles), and flies (fruit flies).",1.0,0.49999999995
What kind of ablation study was performed for the video representation learning methods?,"For each video representation learning method, we perform an ablation study on the key hyperparameter for the respective method and its effect on downstream task performance.","For each video representation learning method, we perform an ablation study on the key hyperparameter for the respective method and its effect on downstream task performance.","['more common, we focus on state-of-the-art video representation learning methods in this section. We additionally\ncompare both community contributed video and trajectory\nrepresentation learning methods. For each video representation learning method, we perform an ablation study on the\nkey hyperparameter for the respective method and its effect\non downstream task performance (Sections 5.3, 5.4), as well\nas pre-training on human datasets (Section 5.5). Finally,\nwe present results from community-contributed methods\non all datasets (Section 5.6), with additional results for the\ntrajectory methods in Appendix F.\n6', 'from an open competition. Our competition was hosted in\ntwo stages, where stage 1 consisted of the trajectory datasets\nfrom mouse and fly, and stage 2 consisted of video datasets\nfrom mouse and beetle. The test sets were private during the\ncompetition phase, and are now released as part of MABe22.\nWe obtained around 1500 submissions in total at the end\nof the competition, and we summarize the top-performing\nmethod for the mouse, fly, and beetle datasets from this\nprocess for both video and trajectory data, with details for\nall methods in Appendix Section A.\n5. Experiments\nWe perform a large set of experiments to evaluate the performance of representation learning methods on MABe 2022\n(Sections 5.1, 5.2). As video representation methods are\nmore common, we focus on state-of-the-art video representation learning methods in this section. We additionally\ncompare both community contributed video and trajectory', 'also solicit community-contributed methods for video and\ntrajectory representation learning through an open competition (Section 4.2). The representation learned by the models\nis a mapping from each video frame/trajectory entry to a\nlower dimensional vector of fixed size. Here, we assume the\nevaluation tasks are hidden during representation learning.\nWe then use this representation of the data to train a linear\nmodel to classify or regress to target values of the hidden\ndownstream task (Appendix D).\n4.1. Self-supervised Video Representation Learning\nSelf-supervised video representation learning methods rely\non designing pretext tasks that make use of prior knowledge\nabout spatial and temporal information in videos to design\npretext tasks such as temporal coherence (Goroshin et al.,\n2015), temporal ordering (Misra et al., 2016), the motion of\nan object (Agrawal et al., 2015), future prediction (Walker\net al., 2016). Contrastive learning (Chen et al., 2020b; He', 'and genetic mutations. A full list of tasks and the types of\nflies used are in Appendix C.1.\nBesides biological differences, we also include tasks based\non manual annotations of the flies’ behavior for the following social behaviors: any aggressive behavior toward\nanother fly, chasing another fly, any courtship behavior toward another fly, high fencing, wing extension, and wing\nflick. We annotated behaviors sparsely across all videos\nwith human experts using JAABA (Kabra et al., 2013), with\nthe goal of including annotations in a wide variety of flies\nand videos.\n4. Benchmarking & Methods\nWe study how well behavioral representations generated by\nstate-of-the-art self-supervised video representation learning methods are suited for decoding our hidden downstream\nbiological tasks and human annotations (Section 4.1). We\nalso solicit community-contributed methods for video and\ntrajectory representation learning through an open competition (Section 4.2). The representation learned by the models', 'MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nWhile this dataset was designed for development of methods for representation learning, the annotations can also be used for supervised\nlearning tasks.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\nFor example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping,']",What kind of ablation study was performed for the video representation learning methods?,"['more common, we focus on state-of-the-art video representation learning methods in this section. We additionally\ncompare both community contributed video and trajectory\nrepresentation learning methods. For each video representation learning method, we perform an ablation study on the\nkey hyperparameter for the respective method and its effect\non downstream task performance (Sections 5.3, 5.4), as well\nas pre-training on human datasets (Section 5.5). Finally,\nwe present results from community-contributed methods\non all datasets (Section 5.6), with additional results for the\ntrajectory methods in Appendix F.\n6', 'from an open competition. Our competition was hosted in\ntwo stages, where stage 1 consisted of the trajectory datasets\nfrom mouse and fly, and stage 2 consisted of video datasets\nfrom mouse and beetle. The test sets were private during the\ncompetition phase, and are now released as part of MABe22.\nWe obtained around 1500 submissions in total at the end\nof the competition, and we summarize the top-performing\nmethod for the mouse, fly, and beetle datasets from this\nprocess for both video and trajectory data, with details for\nall methods in Appendix Section A.\n5. Experiments\nWe perform a large set of experiments to evaluate the performance of representation learning methods on MABe 2022\n(Sections 5.1, 5.2). As video representation methods are\nmore common, we focus on state-of-the-art video representation learning methods in this section. We additionally\ncompare both community contributed video and trajectory', 'also solicit community-contributed methods for video and\ntrajectory representation learning through an open competition (Section 4.2). The representation learned by the models\nis a mapping from each video frame/trajectory entry to a\nlower dimensional vector of fixed size. Here, we assume the\nevaluation tasks are hidden during representation learning.\nWe then use this representation of the data to train a linear\nmodel to classify or regress to target values of the hidden\ndownstream task (Appendix D).\n4.1. Self-supervised Video Representation Learning\nSelf-supervised video representation learning methods rely\non designing pretext tasks that make use of prior knowledge\nabout spatial and temporal information in videos to design\npretext tasks such as temporal coherence (Goroshin et al.,\n2015), temporal ordering (Misra et al., 2016), the motion of\nan object (Agrawal et al., 2015), future prediction (Walker\net al., 2016). Contrastive learning (Chen et al., 2020b; He', 'and genetic mutations. A full list of tasks and the types of\nflies used are in Appendix C.1.\nBesides biological differences, we also include tasks based\non manual annotations of the flies’ behavior for the following social behaviors: any aggressive behavior toward\nanother fly, chasing another fly, any courtship behavior toward another fly, high fencing, wing extension, and wing\nflick. We annotated behaviors sparsely across all videos\nwith human experts using JAABA (Kabra et al., 2013), with\nthe goal of including annotations in a wide variety of flies\nand videos.\n4. Benchmarking & Methods\nWe study how well behavioral representations generated by\nstate-of-the-art self-supervised video representation learning methods are suited for decoding our hidden downstream\nbiological tasks and human annotations (Section 4.1). We\nalso solicit community-contributed methods for video and\ntrajectory representation learning through an open competition (Section 4.2). The representation learned by the models', 'MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nWhile this dataset was designed for development of methods for representation learning, the annotations can also be used for supervised\nlearning tasks.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\nFor example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping,']","['For each video representation learning method, an ablation varies that method’s key hyperparameter and measures the resulting impact on downstream task performance.']","For each video representation learning method, we perform an ablation study on the key hyperparameter for the respective method and its effect on downstream task performance.","For each video representation learning method, we perform an ablation study on the key hyperparameter for the respective method and its effect on downstream task performance.",1.0,0.9999999999
What dimensionality (D) is used for the learned representations across different methods?,"For video models, D is 128. For trajectory methods, D is 128 for mice and 256 for flies.","For video models, D is 128. For trajectory methods, D is 128 for mice and 256 for flies.","['MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nD. Evaluation\nFor all tasks, we evaluate representation learning performance using a linear evaluation protocol, by training a linear model\non top of the learned representation at each frame for classification and regression on a set of downstream tasks. These\ndownstream tasks are unseen during training of the representation learning model. We train separate linear models per task,\nand because of the high class imbalance of some tasks, the classes are weighted inverse to class frequencies during training.\nFor training the linear models, we use three fixed random 80% of the evaluation train split to train three models. All\nevaluations are performed on a fixed test set. For classification tasks, majority voting combines the predictions of the three\nclassifiers. For regression tasks, the predictions are averaged. Both merging schemes are done at the frame level. The', 'dimension is 128.\nMouse Dataset. Different from ant-beetles video data, we utilize SimCLR (Chen et al., 2020b) to extract visual features\nfrom mouse video data. We directly regard other samples in the batch as negative samples instead of constructing a negative\n17', 'best-performing model is in bold.\n5.1. Evaluation Procedure\nFrom an input sequence of video/trajectory data of N\nframes (N = 1800 for mice and 4500 for flies), we evaluate\nmodels that produce learned representations of size N × D,\nwhere D is the dimensionality of the representations. For\nvideo representation learning models, we use D = 128. For\ntrajectory methods, we use D = 128 for mice and D = 256\nfor flies. We then use these feature vectors or embeddings as\ninputs for a linear model that is used to classify/regress the\nhidden task. We use linear least squares with l2 regularized\n(Ridge) classification/regression as model and F1/meansquared-error (MSE) as evaluation metrics (See Appendix D\nfor details).\nWe evaluate a set of state-of-the-art video representation\nlearning methods on MABe 2022, including Masked Autoencoder (MAE) (Feichtenhofer et al., 2022) with a ViT-B\nbackbone (Vaswani et al., 2017), MaskFeat (Wei et al., 2022)\nwith a MViTv2-S backbone (Li et al., 2022) and ρBYOL', 'representation learning models to meaningful but not complete selection of state-of-the-art methods. This gap will be filled\nby the community if our benchmark is adopted to evaluate new methods.\nBroader impact. While the ”quality” of a learned representation will ultimately depend the downstream use, we provide\na resource for the assessment of representation utility by scoring learned representations on a large array of hidden tasks,\nbased on common scientific applications. We note that methods that perform best on our benchmark are not guaranteed to\nbe the best choice for all possible downstream uses of representation learning. Depending on the downstream use, model\ndevelopers may want to consider different choices of architecture, learning objective, and dataset to optimize for different\nproperties of the representation. Our goal is to provide a unified set of tasks across a range of behavior analysis settings', 'n/a\nAny other comments?\nNone.\nUses\nHas the dataset been used for any tasks already? If so, please provide a description.\nYes: this dataset was released to accompany the 2022 Multi-Agent Behavior (MABe) Challenge, posted here. This competition was aimed\nat generating learned representations of animals’ actions using unsupervised or self-supervised techniques.\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nWhile this dataset was designed for development of methods for representation learning, the annotations can also be used for supervised\nlearning tasks.\n28']",What dimensionality (D) is used for the learned representations across different methods?,"['MABe22: Multi-Species Multi-Task Benchmark for Learned Representations of Behavior\nD. Evaluation\nFor all tasks, we evaluate representation learning performance using a linear evaluation protocol, by training a linear model\non top of the learned representation at each frame for classification and regression on a set of downstream tasks. These\ndownstream tasks are unseen during training of the representation learning model. We train separate linear models per task,\nand because of the high class imbalance of some tasks, the classes are weighted inverse to class frequencies during training.\nFor training the linear models, we use three fixed random 80% of the evaluation train split to train three models. All\nevaluations are performed on a fixed test set. For classification tasks, majority voting combines the predictions of the three\nclassifiers. For regression tasks, the predictions are averaged. Both merging schemes are done at the frame level. The', 'dimension is 128.\nMouse Dataset. Different from ant-beetles video data, we utilize SimCLR (Chen et al., 2020b) to extract visual features\nfrom mouse video data. We directly regard other samples in the batch as negative samples instead of constructing a negative\n17', 'best-performing model is in bold.\n5.1. Evaluation Procedure\nFrom an input sequence of video/trajectory data of N\nframes (N = 1800 for mice and 4500 for flies), we evaluate\nmodels that produce learned representations of size N × D,\nwhere D is the dimensionality of the representations. For\nvideo representation learning models, we use D = 128. For\ntrajectory methods, we use D = 128 for mice and D = 256\nfor flies. We then use these feature vectors or embeddings as\ninputs for a linear model that is used to classify/regress the\nhidden task. We use linear least squares with l2 regularized\n(Ridge) classification/regression as model and F1/meansquared-error (MSE) as evaluation metrics (See Appendix D\nfor details).\nWe evaluate a set of state-of-the-art video representation\nlearning methods on MABe 2022, including Masked Autoencoder (MAE) (Feichtenhofer et al., 2022) with a ViT-B\nbackbone (Vaswani et al., 2017), MaskFeat (Wei et al., 2022)\nwith a MViTv2-S backbone (Li et al., 2022) and ρBYOL', 'representation learning models to meaningful but not complete selection of state-of-the-art methods. This gap will be filled\nby the community if our benchmark is adopted to evaluate new methods.\nBroader impact. While the ”quality” of a learned representation will ultimately depend the downstream use, we provide\na resource for the assessment of representation utility by scoring learned representations on a large array of hidden tasks,\nbased on common scientific applications. We note that methods that perform best on our benchmark are not guaranteed to\nbe the best choice for all possible downstream uses of representation learning. Depending on the downstream use, model\ndevelopers may want to consider different choices of architecture, learning objective, and dataset to optimize for different\nproperties of the representation. Our goal is to provide a unified set of tasks across a range of behavior analysis settings', 'n/a\nAny other comments?\nNone.\nUses\nHas the dataset been used for any tasks already? If so, please provide a description.\nYes: this dataset was released to accompany the 2022 Multi-Agent Behavior (MABe) Challenge, posted here. This competition was aimed\nat generating learned representations of animals’ actions using unsupervised or self-supervised techniques.\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nPapers that use or cite this dataset may be submitted by their authors for display on the MABe22 website at\nhttps://sites.google.com/view/computational-behavior/our-datasets/mabe2022-dataset\nWhat (other) tasks could the dataset be used for?\nWhile this dataset was designed for development of methods for representation learning, the annotations can also be used for supervised\nlearning tasks.\n28']","['The learned representation dimensionality is D=128 for video models. For trajectory methods, D=128 for mice and D=256 for flies.']","For video models, D is 128. For trajectory methods, D is 128 for mice and 256 for flies.","For video models, D is 128. For trajectory methods, D is 128 for mice and 256 for flies.",1.0,0.5833333333041666
Which state-of-the-art video representation models and backbones are evaluated?,"The methods include Masked Autoencoder (MAE) with ViT-B, MaskFeat with MViTv2-S, and pBYOL with a SlowFast backbone.","The methods include Masked Autoencoder (MAE) with ViT-B, MaskFeat with MViTv2-S, and pBYOL with a SlowFast backbone.","['backbone (Vaswani et al., 2017), MaskFeat (Wei et al., 2022)\nwith a MViTv2-S backbone (Li et al., 2022) and ρBYOL\n(Feichtenhofer et al., 2021) with a SlowFast backbone (Slow\npathway 8x8) (Frankenhuis et al., 2019). We trained each\nmethod on our mice and beetle data, respectively, as well\nas used backbones pre-trained on human kinetics 400 (Kay\net al., 2017). For implementation details and hyperparameters see Appendix E.\n5.2. Video Representation Results\nWe compare the performance of video representation learning methods on the mouse and beetle video datasets (Table\n2). We find that the pre-trained ρBYOL (R-50 (Slow Pathway) 8x8 model performs best for all action recognition\ntasks (Manuel Behaviors). For all other downstream tasks\ntraining, a ViT-B 16x4 Masked Autoencoder (MAE) that is\nnot pre-trained on Kinetics400 generally performs the best.\nThis top performing MAE architecture uses spatio-temporal\n7', 'best-performing model is in bold.\n5.1. Evaluation Procedure\nFrom an input sequence of video/trajectory data of N\nframes (N = 1800 for mice and 4500 for flies), we evaluate\nmodels that produce learned representations of size N × D,\nwhere D is the dimensionality of the representations. For\nvideo representation learning models, we use D = 128. For\ntrajectory methods, we use D = 128 for mice and D = 256\nfor flies. We then use these feature vectors or embeddings as\ninputs for a linear model that is used to classify/regress the\nhidden task. We use linear least squares with l2 regularized\n(Ridge) classification/regression as model and F1/meansquared-error (MSE) as evaluation metrics (See Appendix D\nfor details).\nWe evaluate a set of state-of-the-art video representation\nlearning methods on MABe 2022, including Masked Autoencoder (MAE) (Feichtenhofer et al., 2022) with a ViT-B\nbackbone (Vaswani et al., 2017), MaskFeat (Wei et al., 2022)\nwith a MViTv2-S backbone (Li et al., 2022) and ρBYOL', 'more common, we focus on state-of-the-art video representation learning methods in this section. We additionally\ncompare both community contributed video and trajectory\nrepresentation learning methods. For each video representation learning method, we perform an ablation study on the\nkey hyperparameter for the respective method and its effect\non downstream task performance (Sections 5.3, 5.4), as well\nas pre-training on human datasets (Section 5.5). Finally,\nwe present results from community-contributed methods\non all datasets (Section 5.6), with additional results for the\ntrajectory methods in Appendix F.\n6', 'and genetic mutations. A full list of tasks and the types of\nflies used are in Appendix C.1.\nBesides biological differences, we also include tasks based\non manual annotations of the flies’ behavior for the following social behaviors: any aggressive behavior toward\nanother fly, chasing another fly, any courtship behavior toward another fly, high fencing, wing extension, and wing\nflick. We annotated behaviors sparsely across all videos\nwith human experts using JAABA (Kabra et al., 2013), with\nthe goal of including annotations in a wide variety of flies\nand videos.\n4. Benchmarking & Methods\nWe study how well behavioral representations generated by\nstate-of-the-art self-supervised video representation learning methods are suited for decoding our hidden downstream\nbiological tasks and human annotations (Section 4.1). We\nalso solicit community-contributed methods for video and\ntrajectory representation learning through an open competition (Section 4.2). The representation learned by the models', 'from an open competition. Our competition was hosted in\ntwo stages, where stage 1 consisted of the trajectory datasets\nfrom mouse and fly, and stage 2 consisted of video datasets\nfrom mouse and beetle. The test sets were private during the\ncompetition phase, and are now released as part of MABe22.\nWe obtained around 1500 submissions in total at the end\nof the competition, and we summarize the top-performing\nmethod for the mouse, fly, and beetle datasets from this\nprocess for both video and trajectory data, with details for\nall methods in Appendix Section A.\n5. Experiments\nWe perform a large set of experiments to evaluate the performance of representation learning methods on MABe 2022\n(Sections 5.1, 5.2). As video representation methods are\nmore common, we focus on state-of-the-art video representation learning methods in this section. We additionally\ncompare both community contributed video and trajectory']",Which state-of-the-art video representation models and backbones are evaluated?,"['backbone (Vaswani et al., 2017), MaskFeat (Wei et al., 2022)\nwith a MViTv2-S backbone (Li et al., 2022) and ρBYOL\n(Feichtenhofer et al., 2021) with a SlowFast backbone (Slow\npathway 8x8) (Frankenhuis et al., 2019). We trained each\nmethod on our mice and beetle data, respectively, as well\nas used backbones pre-trained on human kinetics 400 (Kay\net al., 2017). For implementation details and hyperparameters see Appendix E.\n5.2. Video Representation Results\nWe compare the performance of video representation learning methods on the mouse and beetle video datasets (Table\n2). We find that the pre-trained ρBYOL (R-50 (Slow Pathway) 8x8 model performs best for all action recognition\ntasks (Manuel Behaviors). For all other downstream tasks\ntraining, a ViT-B 16x4 Masked Autoencoder (MAE) that is\nnot pre-trained on Kinetics400 generally performs the best.\nThis top performing MAE architecture uses spatio-temporal\n7', 'best-performing model is in bold.\n5.1. Evaluation Procedure\nFrom an input sequence of video/trajectory data of N\nframes (N = 1800 for mice and 4500 for flies), we evaluate\nmodels that produce learned representations of size N × D,\nwhere D is the dimensionality of the representations. For\nvideo representation learning models, we use D = 128. For\ntrajectory methods, we use D = 128 for mice and D = 256\nfor flies. We then use these feature vectors or embeddings as\ninputs for a linear model that is used to classify/regress the\nhidden task. We use linear least squares with l2 regularized\n(Ridge) classification/regression as model and F1/meansquared-error (MSE) as evaluation metrics (See Appendix D\nfor details).\nWe evaluate a set of state-of-the-art video representation\nlearning methods on MABe 2022, including Masked Autoencoder (MAE) (Feichtenhofer et al., 2022) with a ViT-B\nbackbone (Vaswani et al., 2017), MaskFeat (Wei et al., 2022)\nwith a MViTv2-S backbone (Li et al., 2022) and ρBYOL', 'more common, we focus on state-of-the-art video representation learning methods in this section. We additionally\ncompare both community contributed video and trajectory\nrepresentation learning methods. For each video representation learning method, we perform an ablation study on the\nkey hyperparameter for the respective method and its effect\non downstream task performance (Sections 5.3, 5.4), as well\nas pre-training on human datasets (Section 5.5). Finally,\nwe present results from community-contributed methods\non all datasets (Section 5.6), with additional results for the\ntrajectory methods in Appendix F.\n6', 'and genetic mutations. A full list of tasks and the types of\nflies used are in Appendix C.1.\nBesides biological differences, we also include tasks based\non manual annotations of the flies’ behavior for the following social behaviors: any aggressive behavior toward\nanother fly, chasing another fly, any courtship behavior toward another fly, high fencing, wing extension, and wing\nflick. We annotated behaviors sparsely across all videos\nwith human experts using JAABA (Kabra et al., 2013), with\nthe goal of including annotations in a wide variety of flies\nand videos.\n4. Benchmarking & Methods\nWe study how well behavioral representations generated by\nstate-of-the-art self-supervised video representation learning methods are suited for decoding our hidden downstream\nbiological tasks and human annotations (Section 4.1). We\nalso solicit community-contributed methods for video and\ntrajectory representation learning through an open competition (Section 4.2). The representation learned by the models', 'from an open competition. Our competition was hosted in\ntwo stages, where stage 1 consisted of the trajectory datasets\nfrom mouse and fly, and stage 2 consisted of video datasets\nfrom mouse and beetle. The test sets were private during the\ncompetition phase, and are now released as part of MABe22.\nWe obtained around 1500 submissions in total at the end\nof the competition, and we summarize the top-performing\nmethod for the mouse, fly, and beetle datasets from this\nprocess for both video and trajectory data, with details for\nall methods in Appendix Section A.\n5. Experiments\nWe perform a large set of experiments to evaluate the performance of representation learning methods on MABe 2022\n(Sections 5.1, 5.2). As video representation methods are\nmore common, we focus on state-of-the-art video representation learning methods in this section. We additionally\ncompare both community contributed video and trajectory']","['Evaluated state-of-the-art video representation approaches include MAE with a ViT-B backbone, MaskFeat with an MViTv2-S backbone, and ρBYOL using a SlowFast backbone.']","The methods include Masked Autoencoder (MAE) with ViT-B, MaskFeat with MViTv2-S, and pBYOL with a SlowFast backbone.","The methods include Masked Autoencoder (MAE) with ViT-B, MaskFeat with MViTv2-S, and pBYOL with a SlowFast backbone.",1.0,0.99999999995
